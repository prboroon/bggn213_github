---
title: "Class08MiniProject"
author: "Parnaz Boroon PID: A13557370"
format: pdf
toc: true
---

## Background

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in the last class. You’ll extend what we learned by combining PCA as a pre-processing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.

## Data Import

The data is available as a CSV from the class website:

```{r}
read.csv("WisconsinCancer.csv")
```

Make sure we do not include patient or sample ID or the diagnosis. 

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]

# Create diagnosis vector for later 
diagnosis <- as.factor(wisc.df$diagnosis)
```

## QUESTION 1: How many observations are in this dataset? - 569
```{r}
nrow(wisc.df)
```

## QUESTION 2: How many of the observations have a malignant diagnosis? - 212
```{r}
table(diagnosis)
```

## QUESTION 3: How many variables/features in the data are suffixed with _mean? - 10
```{r}
length(grep("_mean$", names(wisc.df)))
```
# Check column means and standard deviations
```{r}
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)

# Look at summary of results
summary(wisc.pr)
```

## QUESTION 4:From your results, what proportion of the original variance is captured by the first principal components (PC1)? 
44.27%

## QUESTION 5: How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
3 principal components

## QUESTION 6: How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7 principal components

## QUESTION 7: What stands out to you about this plot? Is it easy or difficult to understand? Why?
This plot (biplot) is incredibly difficult to understand as it just shows a huge blob/cluster that is difficult to interpret. The whole point of the PCA plot is to visualize the trend of observations and variables and the graph before me is very difficult to interpret. 

```{r}
diagnosis <- factor(wisc.df$diagnosis,
                    levels = c("B", "M"),
                    labels = c("Benign", "Malignant"))

wisc.pr.df <- as.data.frame(wisc.pr$x)
wisc.pr.df$diagnosis <- diagnosis

library(ggplot2)

ggplot(wisc.pr.df, aes(x = PC1, y = PC2, color = diagnosis)) +
  geom_point(size = 3, alpha = 0.9) +
  scale_color_manual(values = c("Benign" = "hotpink", "Malignant" = "black")) +
  labs(
    title = "PCA of Wisconsin Breast Cancer Data",
    x = "Principal Component 1",
    y = "Principal Component 2",
    color = "Diagnosis"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "right"
  )
```

## QUESTION 8: Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
PC1 vs. PC2 plot shows a wider overview of separation and a view of the two benign and malignant clusters while PC1 vs. PC3 plot shows subtle changes in the spread of clusters.

```{r}
ggplot(wisc.pr.df, aes(x = PC1, y = PC3, color = diagnosis)) +
  geom_point(size = 3, alpha = 0.9) +
  scale_color_manual(values = c("Benign" = "hotpink", "Malignant" = "black")) +
  labs(
    title = "PCA of Wisconsin Breast Cancer Data (PC1 vs PC3)",
    x = "Principal Component 1",
    y = "Principal Component 3",
    color = "Diagnosis"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "right"
  )
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, 
     xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), 
     type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
## QUESTION 9: For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.
```{r}
# Loading (weight) of concave.points_mean on the first principal component
wisc.pr$rotation["concave.points_mean", 1]

```
-0.2608538 means that when concave.points_mean increases, the value of PC1 tends to decrease. Higher concave.points_mean values is associated with malignant samples because malignant samples have more irregular concave borders. 

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
# Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.
data.dist <- dist(data.scaled)
```

```{r}
# Perform hierarchical clustering using complete linkage
wisc.hclust <- hclust(data.dist, method = "complete")
```

## QUESTION 10: Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=18.5, col="red", lty=2)
```

```{r}
# Cut the hierarchical clustering tree into 4 clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.sing <- hclust(data.dist, method="single")
plot(wisc.hclust.sing)

wisc.hclust.sing <- hclust(data.dist, method="complete")
plot(wisc.hclust.sing)

wisc.hclust.sing <- hclust(data.dist, method="average")
plot(wisc.hclust.sing)

wisc.hclust.sing <- hclust(data.dist, method="ward.D2")
plot(wisc.hclust.sing)
```

## QUESTION 12: Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

The method that gave me my favorite results was the ward.D2 method because it merges clusters to minimize the increase in total within-cluster variance. I like being able to visualize more compact, similarly-sized clusters.

```{r}
# Determine how many PCs explain at least 90% variance
summary(wisc.pr)

# Extract those components
wisc.pr.data <- wisc.pr$x[, 1:7]

# Compute Euclidean distances on those PC scores
wisc.pr.dist <- dist(wisc.pr.data)

# Perform hierarchical clustering using Ward’s method
wisc.pr.hclust <- hclust(wisc.pr.dist, method = "ward.D2")
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```
```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```
```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

```

## QUESTION 13 How well does the newly created model with four clusters separate out the two diagnoses?
Separates them very well, there isn't a huge overlap between the two clusters.
```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```

## QUESTION 14 How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.
```{r}
# Compare hierarchical clustering (before PCA) to diagnosis
table(wisc.hclust.clusters, diagnosis)
```

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

## QUESTION 16 Which of these new patients should we prioritize for follow up based on your results?
We should prioritize the malignant group because it is more varied in their tumor cells.

```{r}
sessionInfo()
```

